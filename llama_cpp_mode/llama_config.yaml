# llama_config.yaml

# Model configuration
model_path: "Llama3-8B-Chinese-Chat-q8_0-v2_1.gguf"
verbose: false
n_gpu_layers: 0  # Set to 0 to run on CPU only
n_ctx: 4096  # Context window size
main_gpu: 0
tensor_split: null  # Ensure model is not split across multiple GPUs
offload_kqv: false  # Ensure K, Q, V are not offloaded to GPU
flash_attn: false  # Disable flash attention for CPU compatibility

# Generation parameters
max_tokens: 1024
seed: 42
temperature: 0.0
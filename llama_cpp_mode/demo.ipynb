{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1fa278e-4844-4b88-9c8c-5f86bac6c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp_client import LlamaCppClient\n",
    "\n",
    "client = LlamaCppClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d4f4606-d296-452e-ba58-3e8b6837ac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 09:19:56,366 - INFO - Execution Time: 38.38 seconds\n",
      "2024-08-02 09:19:56,367 - INFO - Input Word Count: 0\n",
      "2024-08-02 09:19:56,367 - INFO - Output Word Count: 410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on enabling computers to understand and interpret human language. Over the past few years, there have been significant advancements in NLP, leading to more accurate and efficient language understanding systems.\n",
      "\n",
      "1. Deep Learning: One of the most significant advancements in NLP has been the adoption of deep learning techniques, particularly recurrent neural networks (RNNs) and their variants such as long short-term memory (LSTM) and gated recurrent units (GRU). These models have shown remarkable improvements in tasks like language modeling, machine translation, and sentiment analysis.\n",
      "\n",
      "2. Transformer Models: Introduced by Vaswani et al. in 2017, transformer models have revolutionized the field of NLP. They are based on self-attention mechanisms that allow for parallel processing of input sequences, making them much faster than traditional RNNs. The most popular transformer model is BERT (Bidirectional Encoder Representations from Transformers), which has achieved state-of-the-art results in a wide range of NLP tasks.\n",
      "\n",
      "3. Pre-trained Language Models: Another significant advancement in NLP is the development of pre-trained language models like GPT-2, GPT-3, and XLNet. These models are trained on large amounts of text data and can be fine-tuned for specific NLP tasks. They have shown impressive results in areas such as text generation, question answering, and natural language inference.\n",
      "\n",
      "4. Multimodal Learning: With the increasing availability of multimedia content, there has been a growing interest in multimodal learning, which involves processing both text and visual data together. Models like BERT-VIS and VisualBERT have demonstrated promising results in tasks that require understanding both textual and visual information.\n",
      "\n",
      "5. Explainability and Interpretability: As NLP models become more complex and accurate, there is an increasing need for explainable AI (XAI) techniques to understand how these models arrive at their predictions. Techniques like attention mechanisms and saliency maps have been developed to provide insights into the decision-making process of NLP models.\n",
      "\n",
      "6. Multilingual Models: With the growing importance of global communication, there has been a focus on developing multilingual language models that can handle multiple languages simultaneously. Models like mBERT (Multilingual BERT) and XLM-R (Cross-lingual Language Model Representations) have shown promising results in this area.\n",
      "\n",
      "These advancements in NLP have led to more accurate and efficient language understanding systems, enabling applications such as chatbots, virtual assistants, and language translation tools. As the field continues to evolve, we can expect even more sophisticated models that can better understand human language and interact with humans in a more natural way.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are an AI expert.\"\n",
    "user_prompt = \"Explain the latest advancements in natural language processing.\"\n",
    "\n",
    "response = client.generate_response(system_prompt, user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b332de-87a5-453a-8f61-d51d2b8876ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e8fbe5-94a0-4d8f-8441-178bfedb7153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

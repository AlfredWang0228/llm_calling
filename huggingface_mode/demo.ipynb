{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12766e34-1dc4-4685-bde4-fb2e5bea5667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_llm_client import HuggingFaceLLMClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e4566a-2ca2-4408-8ed1-dd65443c9a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f6fb12c5b4468ab3d8c9c8ba5493f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = HuggingFaceLLMClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "777e959e-6fc4-4a00-8c39-b90bfe96ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You're a helpful assistant.\"\n",
    "user_prompt = \"What is the architecture of LLaMa-3?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b0b5df7-6e04-4f9d-be7c-863154c37fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 08:30:45,473 - INFO - Execution Time: 87.70 seconds\n",
      "2024-08-02 08:30:45,473 - INFO - Input Word Count: 10\n",
      "2024-08-02 08:30:45,474 - INFO - Output Word Count: 418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-3 is a large language model developed by Meta AI, and its architecture is based on the transformer model introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017.\n",
      "\n",
      "Here's a high-level overview of the architecture:\n",
      "\n",
      "1. **Encoder**: The encoder is a stack of identical layers, each consisting of two sub-layers: a self-attention mechanism and a feed-forward neural network (FFNN). The self-attention mechanism allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. The FFNN is used to transform the output of the self-attention mechanism.\n",
      "2. **Decoder**: The decoder is also a stack of identical layers, similar to the encoder. However, the decoder has an additional sub-layer that generates the output sequence. The decoder takes the output from the encoder and generates the output sequence one token at a time.\n",
      "3. **Multi-head attention**: LLaMA-3 uses multi-head attention, which allows the model to jointly attend to information from different representation subspaces at different positions. This is achieved by dividing the input into multiple attention heads, each of which is applied to a different subset of the input.\n",
      "4. **Positional encoding**: LLaMA-3 uses positional encoding to provide the model with information about the position of each token in the input sequence. This is necessary because the transformer model does not have recurrence or convolution, which are typically used to capture sequential relationships in the input.\n",
      "5. **Layer normalization**: LLaMA-3 uses layer normalization to normalize the output of each layer. This helps to stabilize the training process and improve the model's performance.\n",
      "6. **Dropout**: LLaMA-3 uses dropout to randomly drop out neurons during training. This helps to prevent overfitting and improve the model's generalization performance.\n",
      "7. **Output layer**: The output layer is a linear layer that takes the output from the decoder and generates the final output sequence.\n",
      "\n",
      "LLaMA-3 has several key differences from other transformer-based language models, including:\n",
      "\n",
      "* **Large model size**: LLaMA-3 has a large model size, with over 1 billion parameters. This allows the model to capture complex patterns in the input data.\n",
      "* **Customized architecture**: LLaMA-3 has a customized architecture that is designed to improve the model's performance on specific tasks, such as language translation and text generation.\n",
      "* **Pre-training**: LLaMA-3 is pre-trained on a large corpus of text data, which allows the model to learn general language representations that can be fine-tuned for specific tasks.\n",
      "\n",
      "Overall, the architecture of LLaMA-3 is designed to capture complex patterns in language data and generate high-quality text output.\n"
     ]
    }
   ],
   "source": [
    "answer = client.generate_text(system_prompt, user_prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8332cda7-d103-4459-aa50-f33170edc24d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
